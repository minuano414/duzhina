{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2bCT9ZmOXM_D",
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.0 in ./venv/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision==0.15.1 in ./venv/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.9/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: ultralytics in ./venv/lib/python3.9/site-packages (8.0.90)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from torch==2.0.0) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.9/site-packages (from torch==2.0.0) (4.5.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.9/site-packages (from torch==2.0.0) (3.1)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.9/site-packages (from torch==2.0.0) (1.11.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from torch==2.0.0) (3.12.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.9/site-packages (from torchvision==0.15.1) (9.5.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (from torchvision==0.15.1) (1.24.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from torchvision==0.15.1) (2.28.2)\n",
      "Requirement already satisfied: thop>=0.1.1 in ./venv/lib/python3.9/site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in ./venv/lib/python3.9/site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.9/site-packages (from ultralytics) (5.9.4)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./venv/lib/python3.9/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in ./venv/lib/python3.9/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in ./venv/lib/python3.9/site-packages (from ultralytics) (3.7.1)\n",
      "Requirement already satisfied: sentry-sdk in ./venv/lib/python3.9/site-packages (from ultralytics) (1.21.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in ./venv/lib/python3.9/site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./venv/lib/python3.9/site-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests->torchvision==0.15.1) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests->torchvision==0.15.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests->torchvision==0.15.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests->torchvision==0.15.1) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.9/site-packages (from jinja2->torch==2.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.9/site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.1.1 is available.\n",
      "You should consider upgrading via the '/Users/bfst/PycharmProjects/airflow-dwh/venv/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.0 torchvision==0.15.1 opencv-python ultralytics scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://readonly:****@pypi.revolutlabs.com/simple/, https://pypi.ngc.nvidia.com, https://data:****@pypi.revolutlabs.com/simple, https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.10/site-packages (0.14.1)\n",
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105b45a50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torchaudio/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105b45d50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torchaudio/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105b45f00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torchaudio/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105b460b0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torchaudio/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105b46260>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torchaudio/\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting torchaudio\n",
      "  Downloading torchaudio-2.0.1-cp310-cp310-macosx_11_0_arm64.whl (3.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m31.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from torchvision) (1.23.3)\n",
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105f7a1d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torch/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105f7a380>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torch/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105f7a980>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torch/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105f7ab30>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torch/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x105f7ace0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/torch/\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting torch\n",
      "  Downloading torch-2.0.0-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 MB\u001B[0m \u001B[31m60.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.0.3)\n",
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078b6470>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/networkx/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078b66e0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/networkx/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078b6890>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/networkx/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078b6a40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/networkx/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078b6bf0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/networkx/\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m48.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078e7b80>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/sympy/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078e7df0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/sympy/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1078e7fd0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/sympy/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107d241c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/sympy/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107d24370>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/sympy/\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting sympy\n",
      "  Downloading sympy-1.12rc1-py3-none-any.whl (5.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m46.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/ilya.slepov/Library/Python/3.10/lib/python/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision) (2022.6.15.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107da8070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/mpmath/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107da82e0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/mpmath/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107da8490>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/mpmath/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107da8640>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/mpmath/\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107da87f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /whl/nightly/cpu/mpmath/\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m74.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: mpmath, sympy, networkx, torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "Successfully installed mpmath-1.3.0 networkx-3.1 sympy-1.12rc1 torch-2.0.0 torchaudio-2.0.1\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3.10 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://readonly:****@pypi.revolutlabs.com/simple/, https://pypi.ngc.nvidia.com, https://data:****@pypi.revolutlabs.com/simple\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m138.3/138.3 kB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: ipython>=6.1.0 in /opt/homebrew/lib/python3.10/site-packages (from ipywidgets) (8.5.0)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Downloading jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m198.2/198.2 kB\u001B[0m \u001B[31m24.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/homebrew/lib/python3.10/site-packages (from ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/homebrew/lib/python3.10/site-packages (from ipywidgets) (6.16.0)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m34.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: jupyter-client>=6.1.12 in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.5)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/homebrew/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: backcall in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.31)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/homebrew/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/homebrew/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/homebrew/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.11.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: executing in /opt/homebrew/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: asttokens in /opt/homebrew/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.0.6 jupyterlab-widgets-3.0.7 widgetsnbextension-4.0.7\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3.10 -m pip install --upgrade pip\u001B[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: problems found:\n",
      "        - require? \u001B[31m X\u001B[0m jupyter-js-widgets/extension\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "KEe_jJNKWkZq",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import shutil\n",
    "import itertools\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from torch.optim import Adam\n",
    "from torchvision.ops import nms\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4FJVPSmXb5M",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Yolo fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RYkieFW3cqai",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
      "100%|████████████████████████████████████████| 6.23M/6.23M [00:00<00:00, 23.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "yolo_model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFQhOi1wXwmP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ActorsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for label, class_dir in enumerate(os.listdir(self.root_dir)):\n",
    "            class_path = os.path.join(self.root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_file in os.listdir(class_path):\n",
    "                    img_path = os.path.join(class_path, img_file)\n",
    "                    if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        samples.append((img_path, label))\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"/Users/ilya.slepov/Downloads/actor_fix\"\n",
    "\n",
    "train_dir = \"./Dataset/train/\"\n",
    "test_dir = \"./Dataset/test/\"\n",
    "\n",
    "def split_dataset(data_dir, train_dir, test_dir, test_size=0.2, random_state=123):\n",
    "    actor_folders = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    for actor_folder in actor_folders:\n",
    "        actor_path = os.path.join(data_dir, actor_folder)\n",
    "        train_actor_path = os.path.join(train_dir, actor_folder)\n",
    "        test_actor_path = os.path.join(test_dir, actor_folder)\n",
    "        \n",
    "        os.makedirs(train_actor_path, exist_ok=True)\n",
    "        os.makedirs(test_actor_path, exist_ok=True)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(actor_path) if f.endswith('.png')]\n",
    "        n_samples = len(image_files)\n",
    "        \n",
    "        train_files, test_files = [],[]\n",
    "        \n",
    "        if n_samples == 1:\n",
    "            train_files, test_files = image_files, image_files\n",
    "        else:\n",
    "            current_test_size = test_size\n",
    "            train_files, test_files = train_test_split(image_files, test_size=current_test_size, random_state=random_state)\n",
    "            \n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(actor_path, file), os.path.join(test_actor_path, file))\n",
    "        \n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(actor_path, file), os.path.join(train_actor_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_dataset(data_dir, train_dir, test_dir, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = ActorsDataset(root_dir=train_dir, transform=transform)\n",
    "test_data = ActorsDataset(root_dir=test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is :: mps\n"
     ]
    }
   ],
   "source": [
    "CPU= False\n",
    "device = \"cpu\" if CPU else torch.device(\"mps\")\n",
    "print(\"Device is :: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /Users/ilya.slepov/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████████████████████████████████| 83.3M/83.3M [00:06<00:00, 14.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(os.listdir(data_dir))\n",
    "device = torch.device(device)\n",
    "\n",
    "model = models.resnet34(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████| 1548/1548 [14:19<00:00,  1.80it/s, Loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7906235437960549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████| 1548/1548 [14:26<00:00,  1.79it/s, Loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.2079825449171801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████| 1548/1548 [14:25<00:00,  1.79it/s, Loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.13560995784015406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████| 1548/1548 [14:23<00:00,  1.79it/s, Loss=0.0975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.09749765375307798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████████| 1548/1548 [14:21<00:00,  1.80it/s, Loss=0.0746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.07462877590279636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████| 1548/1548 [14:23<00:00,  1.79it/s, Loss=0.0602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.06017492467297472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████████| 1548/1548 [14:20<00:00,  1.80it/s, Loss=0.0485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.04845922457940759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████████| 1548/1548 [14:17<00:00,  1.80it/s, Loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.038735875318690806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████████| 1548/1548 [14:16<00:00,  1.81it/s, Loss=0.0326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.032590144633186754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████████████| 1548/1548 [14:16<00:00,  1.81it/s, Loss=0.0279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.027944812194299246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap the train_loader with tqdm to display a progress bar\n",
    "    train_loader_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader_progress):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar with the current loss\n",
    "        train_loader_progress.set_postfix({\"Loss\": running_loss / (i+1)})\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"./checkpoint_{epoch}_resnet_loss_{running_loss / len(train_loader)}.pth\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.10744732185941%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = evaluate_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFoBXkEadEqJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_custom_resnet(model_path):\n",
    "    # Load the ResNet model\n",
    "    model = torchvision.models.resnet34()\n",
    "\n",
    "    # Modify the last layer to match the number of classes in your custom model\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 96)\n",
    "\n",
    "    # Load the model weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_custom_resnet(\"classification_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_V71YOldGX2",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classify_actor(image, bbox):\n",
    "    x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "    image = Image.fromarray(image)\n",
    "    crop_image = image.crop((x1, y1, x2, y2))\n",
    "    crop_image = F.resize(crop_image, (224, 224))\n",
    "    crop_image = F.to_tensor(crop_image)\n",
    "    crop_image = F.normalize(crop_image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        class_probs = torch.softmax(model(crop_image.unsqueeze(0)), dim=1)\n",
    "        top_class_id = class_probs.argmax(dim=1).item()\n",
    "        return top_class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crop(y1, y2, w, h):\n",
    "    max_w = h * 9 / 16\n",
    "    cur_w = y2 - y1\n",
    "    crop_cord = y2\n",
    "    if cur_w < max_w:\n",
    "        crop_cord = crop_cord + abs(max_w - cur_w) // 2\n",
    "    elif cur_w > max_w:\n",
    "        crop_cord = crop_cord - abs(max_w - cur_w) // 2\n",
    "\n",
    "    if crop_cord > w:\n",
    "        crop_cord = w\n",
    "    \n",
    "    return crop_cord - max_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_m02xEaX0r7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### BB choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prioritise(ids, actors_priority):\n",
    "    index_dict = {}\n",
    "    for i in range(len(actors_priority)):\n",
    "        index_dict[actors_priority[i]] = i\n",
    "\n",
    "    min_index = len(actors_priority)\n",
    "    result = None\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        if ids[i] in index_dict:\n",
    "            if index_dict[ids[i]] < min_index:\n",
    "                min_index = index_dict[ids[i]]\n",
    "                result = ids[i]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def is_speaking(boxes, video_metadata):\n",
    "    if not isinstance(video_metadata, list):\n",
    "        video_metadata = [video_metadata]\n",
    "    for meta, box in itertools.product(video_metadata, boxes):\n",
    "        xyxy = box[0].xyxy[0]\n",
    "        x1, y1, x2, y2 = xyxy[0], float(xyxy[1]), xyxy[2], float(xyxy[3])\n",
    "        if meta[\"score\"] > 0 and abs(meta[\"y1\"] - y1) < 100.0 and abs(meta[\"y2\"] - y2) < 100.0:\n",
    "            return box[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "pTHckZmUX2GX",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _box_choise(pred, frame, actors_priority=None, video_metadata=None):\n",
    "        actors = {}\n",
    "        boxes = pred.boxes\n",
    "        \n",
    "        if len(boxes) == 1 or (not actors_priority and not video_metadata):\n",
    "            return boxes[0]\n",
    "        elif len(boxes) == 0:\n",
    "            return None\n",
    "        \n",
    "        for box in boxes:\n",
    "            actor = classify_actor(frame, box[0].xyxy[0])\n",
    "            actors[str(actor)] = box\n",
    "            \n",
    "        score = 0\n",
    "        try:\n",
    "            score = video_metadata[0].get(\"score\", -1)\n",
    "        except:\n",
    "            score = video_metadata.get(\"score\", -1)\n",
    "            \n",
    "        if video_metadata and score > 0:\n",
    "            actor = is_speaking(boxes, video_metadata)\n",
    "            if actor:\n",
    "                return actor\n",
    "        \n",
    "        if actors_priority:\n",
    "            actor = prioritise(list(actors.keys()), actors_priority)\n",
    "            if actor:\n",
    "                return actors[actor]\n",
    "\n",
    "        return boxes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g81uI-NmX7eW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "ilM2aOrbYDsG",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_frame(frame, count, width, height, actors_priorty = None, video_metadata = None):\n",
    "        with torch.no_grad():\n",
    "            pred = yolo_model(frame, iou=0.45, conf=0.75, classes=[0])[0]\n",
    "\n",
    "        actor_box = _box_choise(pred, frame, actors_priorty, video_metadata[count])\n",
    "        if actor_box:\n",
    "            for xyxy in actor_box[0].xyxy:\n",
    "                x1, y1, x2, y2 = xyxy[0], xyxy[1], xyxy[2], xyxy[3]\n",
    "                return crop(y1, y2, width, height)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def process_video(input_video_path, output_video_path, actors_priorty = None, video_metadata = None):\n",
    "    input_video = cv2.VideoCapture(input_video_path)\n",
    "    width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = input_video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_amount = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    output_list = []\n",
    "    count = 0\n",
    "\n",
    "    output_video = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    while input_video.isOpened():\n",
    "        ret, frame = input_video.read()\n",
    "        crop_y = process_frame(frame, count, width, height, actors_priorty, video_metadata)\n",
    "        output_list.append({'frame_number': count, 'crop_coordinates': float(crop_y) if crop_y else None})\n",
    "        count += 1\n",
    "        if not ret or count == frames_amount:\n",
    "            count = 0\n",
    "            break\n",
    "\n",
    "    input_video.release()\n",
    "    output_video.release()\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "yyh07GSHWoty",
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 60.0ms\n",
      "Speed: 1.8ms preprocess, 60.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 406 406\n",
      "10\n",
      "27\n",
      "1 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 46.0ms\n",
      "Speed: 1.4ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.3ms\n",
      "Speed: 1.2ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "27\n",
      "2 406 406\n",
      "10\n",
      "27\n",
      "3 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 47.8ms\n",
      "Speed: 1.2ms preprocess, 47.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.0ms\n",
      "Speed: 1.1ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "27\n",
      "4 406 406\n",
      "33\n",
      "27\n",
      "5 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 43.1ms\n",
      "Speed: 1.1ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 1.8ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "27\n",
      "6 406 406\n",
      "10\n",
      "27\n",
      "7 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 50.8ms\n",
      "Speed: 1.8ms preprocess, 50.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "27\n",
      "8 406 406\n",
      "10\n",
      "27\n",
      "9 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 44.5ms\n",
      "Speed: 1.3ms preprocess, 44.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.0ms\n",
      "Speed: 1.1ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.7ms\n",
      "Speed: 1.0ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "10 406 406\n",
      "27\n",
      "10\n",
      "11 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 43.9ms\n",
      "Speed: 1.3ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.3ms\n",
      "Speed: 1.1ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "29\n",
      "12 406 406\n",
      "48\n",
      "29\n",
      "13 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 43.2ms\n",
      "Speed: 1.2ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 1.1ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "29\n",
      "14 406 406\n",
      "33\n",
      "27\n",
      "15 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 45.3ms\n",
      "Speed: 1.0ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.2ms\n",
      "Speed: 1.2ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "16 406 406\n",
      "10\n",
      "27\n",
      "17 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 46.9ms\n",
      "Speed: 1.3ms preprocess, 46.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 47.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "18 406 406\n",
      "33\n",
      "17\n",
      "19 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.1ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.9ms\n",
      "Speed: 1.3ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.0ms\n",
      "Speed: 1.3ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "29\n",
      "20 406 406\n",
      "10\n",
      "29\n",
      "21 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 45.9ms\n",
      "Speed: 1.3ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "71\n",
      "22 406 406\n",
      "10\n",
      "71\n",
      "23 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 47.6ms\n",
      "Speed: 1.5ms preprocess, 47.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 47.1ms\n",
      "Speed: 1.2ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "29\n",
      "24 406 406\n",
      "10\n",
      "17\n",
      "25 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 46.7ms\n",
      "Speed: 1.2ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 47.0ms\n",
      "Speed: 1.3ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.2ms\n",
      "Speed: 1.2ms preprocess, 46.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "26 406 406\n",
      "10\n",
      "17\n",
      "27 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 48.0ms\n",
      "Speed: 1.2ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.5ms\n",
      "Speed: 1.4ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "28 406 406\n",
      "10\n",
      "17\n",
      "29 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 47.7ms\n",
      "Speed: 1.2ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.5ms\n",
      "Speed: 1.3ms preprocess, 46.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "17\n",
      "30 406 406\n",
      "10\n",
      "17\n",
      "31 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 48.5ms\n",
      "Speed: 1.1ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "62\n",
      "32 406 406\n",
      "17\n",
      "62\n",
      "33 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 48.0ms\n",
      "Speed: 1.4ms preprocess, 48.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 48.5ms\n",
      "Speed: 1.1ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "62\n",
      "34 406 406\n",
      "17\n",
      "10\n",
      "35 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 50.3ms\n",
      "Speed: 1.2ms preprocess, 50.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 48.5ms\n",
      "Speed: 1.3ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "36 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "37 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 48.5ms\n",
      "Speed: 1.2ms preprocess, 48.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 49.2ms\n",
      "Speed: 1.2ms preprocess, 49.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "10\n",
      "17\n",
      "38 406 406\n",
      "10\n",
      "29\n",
      "39 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 51.6ms\n",
      "Speed: 1.4ms preprocess, 51.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 49.5ms\n",
      "Speed: 1.2ms preprocess, 49.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "17\n",
      "40 406 406\n",
      "10\n",
      "29\n",
      "41 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 50.2ms\n",
      "Speed: 1.3ms preprocess, 50.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 48.9ms\n",
      "Speed: 1.3ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "42 406 406\n",
      "62\n",
      "17\n",
      "43 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 49.8ms\n",
      "Speed: 1.2ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 47.0ms\n",
      "Speed: 1.4ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "44 406 406\n",
      "10\n",
      "17\n",
      "45 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 53.4ms\n",
      "Speed: 1.2ms preprocess, 53.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 50.4ms\n",
      "Speed: 1.7ms preprocess, 50.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "46 406 406\n",
      "10\n",
      "17\n",
      "47 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 48.5ms\n",
      "Speed: 1.1ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 50.3ms\n",
      "Speed: 1.2ms preprocess, 50.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "48 406 406\n",
      "10\n",
      "17\n",
      "49 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 51.0ms\n",
      "Speed: 1.5ms preprocess, 51.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 53.1ms\n",
      "Speed: 2.0ms preprocess, 53.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "50 406 406\n",
      "17\n",
      "10\n",
      "51 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 49.3ms\n",
      "Speed: 1.4ms preprocess, 49.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.0ms\n",
      "Speed: 1.2ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.8ms\n",
      "Speed: 1.2ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "52 406 406\n",
      "17\n",
      "10\n",
      "53 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 46.8ms\n",
      "Speed: 1.7ms preprocess, 46.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "54 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "55 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 1.3ms preprocess, 43.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 49.8ms\n",
      "Speed: 1.2ms preprocess, 49.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "17\n",
      "56 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 65.7ms\n",
      "Speed: 1.2ms preprocess, 65.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "57 406 406\n",
      "10\n",
      "17\n",
      "58 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 54.5ms\n",
      "Speed: 1.2ms preprocess, 54.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 47.9ms\n",
      "Speed: 1.2ms preprocess, 47.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "17\n",
      "59 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "60 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 54.9ms\n",
      "Speed: 1.7ms preprocess, 54.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "61 406 406\n",
      "10\n",
      "17\n",
      "62 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 54.1ms\n",
      "Speed: 2.1ms preprocess, 54.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 90.8ms\n",
      "Speed: 1.2ms preprocess, 90.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "33\n",
      "63 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 53.0ms\n",
      "Speed: 1.2ms preprocess, 53.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64 406 406\n",
      "10\n",
      "33\n",
      "65 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 56.9ms\n",
      "Speed: 1.1ms preprocess, 56.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.4ms\n",
      "Speed: 1.1ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.0ms\n",
      "Speed: 1.2ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "66 406 406\n",
      "10\n",
      "17\n",
      "67 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.4ms\n",
      "Speed: 1.2ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "68 406 406\n",
      "10\n",
      "10\n",
      "69 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 85.1ms\n",
      "Speed: 1.4ms preprocess, 85.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 58.2ms\n",
      "Speed: 2.0ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "70 406 406\n",
      "10\n",
      "17\n",
      "71 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 64.4ms\n",
      "Speed: 1.5ms preprocess, 64.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 59.7ms\n",
      "Speed: 1.2ms preprocess, 59.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "72 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 41.5ms\n",
      "Speed: 1.2ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "73 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "74 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 44.8ms\n",
      "Speed: 1.4ms preprocess, 44.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 49.7ms\n",
      "Speed: 1.3ms preprocess, 49.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "75 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 57.4ms\n",
      "Speed: 1.2ms preprocess, 57.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "76 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "77 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 43.9ms\n",
      "Speed: 1.3ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 48.7ms\n",
      "Speed: 1.9ms preprocess, 48.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "27\n",
      "17\n",
      "78 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 49.1ms\n",
      "Speed: 1.4ms preprocess, 49.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "79 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 53.1ms\n",
      "Speed: 1.2ms preprocess, 53.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "80 406 406\n",
      "10\n",
      "17\n",
      "16\n",
      "81 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 49.0ms\n",
      "Speed: 1.3ms preprocess, 49.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 76.0ms\n",
      "Speed: 1.4ms preprocess, 76.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "53\n",
      "82 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 56.1ms\n",
      "Speed: 2.3ms preprocess, 56.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "16\n",
      "83 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 60.0ms\n",
      "Speed: 1.2ms preprocess, 60.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "29\n",
      "84 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 62.8ms\n",
      "Speed: 1.1ms preprocess, 62.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "29\n",
      "85 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 94.0ms\n",
      "Speed: 3.5ms preprocess, 94.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "29\n",
      "86 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 49.3ms\n",
      "Speed: 1.1ms preprocess, 49.3ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "62\n",
      "87 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 53.8ms\n",
      "Speed: 1.2ms preprocess, 53.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "27\n",
      "29\n",
      "88 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 46.1ms\n",
      "Speed: 1.2ms preprocess, 46.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "29\n",
      "89 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 55.5ms\n",
      "Speed: 1.3ms preprocess, 55.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "90 406 406\n",
      "10\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "91 406 406\n",
      "10\n",
      "10\n",
      "29\n",
      "92 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 53.4ms\n",
      "Speed: 1.2ms preprocess, 53.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 48.5ms\n",
      "Speed: 1.2ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "29\n",
      "93 406 406\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 47.4ms\n",
      "Speed: 1.1ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "17\n",
      "94 406 406\n",
      "65\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 56.4ms\n",
      "Speed: 1.8ms preprocess, 56.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "95 406 406\n",
      "65\n",
      "27\n",
      "17\n",
      "96 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 44.3ms\n",
      "Speed: 1.2ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "17\n",
      "17\n",
      "97 406 406\n",
      "17\n",
      "27\n",
      "33\n",
      "98 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 1.1ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 52.2ms\n",
      "Speed: 1.5ms preprocess, 52.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "33\n",
      "17\n",
      "99 406 406\n",
      "27\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "100 406 406\n",
      "95\n",
      "82\n",
      "27\n",
      "101 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 56.5ms\n",
      "Speed: 1.7ms preprocess, 56.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 57.8ms\n",
      "Speed: 1.2ms preprocess, 57.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "29\n",
      "65\n",
      "102 406 406\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 52.8ms\n",
      "Speed: 1.4ms preprocess, 52.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "17\n",
      "103 406 406\n",
      "33\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 53.6ms\n",
      "Speed: 1.7ms preprocess, 53.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "104 406 406\n",
      "33\n",
      "17\n",
      "17\n",
      "105 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.6ms\n",
      "Speed: 1.2ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "29\n",
      "106 406 406\n",
      "10\n",
      "17\n",
      "29\n",
      "107 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 42.0ms\n",
      "Speed: 1.1ms preprocess, 42.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "29\n",
      "108 406 406\n",
      "10\n",
      "17\n",
      "29\n",
      "109 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 44.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "29\n",
      "110 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "111 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.1ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "112 406 406\n",
      "36\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 48.6ms\n",
      "Speed: 1.1ms preprocess, 48.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.7ms\n",
      "Speed: 1.2ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "113 406 406\n",
      "17\n",
      "17\n",
      "27\n",
      "114 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "17\n",
      "17\n",
      "115 406 406\n",
      "29\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.0ms\n",
      "Speed: 1.1ms preprocess, 45.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 43.1ms\n",
      "Speed: 1.2ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "116 406 406\n",
      "17\n",
      "33\n",
      "17\n",
      "117 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 45.0ms\n",
      "Speed: 1.1ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "32\n",
      "17\n",
      "118 406 406\n",
      "17\n",
      "17\n",
      "119 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 46.5ms\n",
      "Speed: 1.1ms preprocess, 46.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "32\n",
      "120 406 406\n",
      "17\n",
      "17\n",
      "10\n",
      "121 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 44.6ms\n",
      "Speed: 1.1ms preprocess, 44.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "122 406 406\n",
      "10\n",
      "17\n",
      "62\n",
      "123 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "124 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "125 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "126 406 406\n",
      "10\n",
      "17\n",
      "16\n",
      "127 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.6ms\n",
      "Speed: 1.2ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64\n",
      "128 406 406\n",
      "10\n",
      "17\n",
      "64\n",
      "129 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 1.2ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "130 406 406\n",
      "10\n",
      "17\n",
      "131 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "132 406 406\n",
      "10\n",
      "17\n",
      "133 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 59.1ms\n",
      "Speed: 1.2ms preprocess, 59.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "134 406 406\n",
      "10\n",
      "17\n",
      "135 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 49.4ms\n",
      "Speed: 1.6ms preprocess, 49.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "136 406 406\n",
      "10\n",
      "17\n",
      "137 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 1.1ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.5ms\n",
      "Speed: 1.1ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 48.1ms\n",
      "Speed: 1.1ms preprocess, 48.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "138 406 406\n",
      "10\n",
      "17\n",
      "139 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 46.9ms\n",
      "Speed: 1.5ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.2ms\n",
      "Speed: 1.1ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "140 406 406\n",
      "10\n",
      "17\n",
      "141 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 46.6ms\n",
      "Speed: 1.2ms preprocess, 46.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "142 406 406\n",
      "10\n",
      "17\n",
      "143 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 47.4ms\n",
      "Speed: 1.5ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "144 406 406\n",
      "10\n",
      "17\n",
      "145 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 46.0ms\n",
      "Speed: 1.2ms preprocess, 46.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 59.6ms\n",
      "Speed: 1.9ms preprocess, 59.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "146 406 406\n",
      "17\n",
      "10\n",
      "147 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 1.2ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 56.8ms\n",
      "Speed: 1.3ms preprocess, 56.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "148 406 406\n",
      "10\n",
      "17\n",
      "149 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 61.4ms\n",
      "Speed: 1.3ms preprocess, 61.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 72.4ms\n",
      "Speed: 1.6ms preprocess, 72.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "150 406 406\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 68.0ms\n",
      "Speed: 1.4ms preprocess, 68.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "151 406 406\n",
      "17\n",
      "10\n",
      "17\n",
      "152 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 94.8ms\n",
      "Speed: 1.4ms preprocess, 94.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 77.1ms\n",
      "Speed: 1.2ms preprocess, 77.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "17\n",
      "153 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "17\n",
      "154 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 107.0ms\n",
      "Speed: 1.8ms preprocess, 107.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 54.7ms\n",
      "Speed: 1.5ms preprocess, 54.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "155 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 52.0ms\n",
      "Speed: 1.6ms preprocess, 52.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "64\n",
      "156 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 46.7ms\n",
      "Speed: 1.2ms preprocess, 46.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "157 406 406\n",
      "17\n",
      "10\n",
      "64\n",
      "158 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 46.5ms\n",
      "Speed: 1.3ms preprocess, 46.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 45.5ms\n",
      "Speed: 1.3ms preprocess, 45.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64\n",
      "159 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 66.3ms\n",
      "Speed: 1.7ms preprocess, 66.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "160 406 406\n",
      "17\n",
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.8ms\n",
      "Speed: 1.2ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 46.6ms\n",
      "Speed: 1.8ms preprocess, 46.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "161 406 406\n",
      "10\n",
      "17\n",
      "29\n",
      "162 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 83.9ms\n",
      "Speed: 1.3ms preprocess, 83.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "17\n",
      "29\n",
      "163 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "17\n",
      "64\n",
      "164 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 95.1ms\n",
      "Speed: 1.3ms preprocess, 95.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 49.1ms\n",
      "Speed: 2.2ms preprocess, 49.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "62\n",
      "64\n",
      "165 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 52.6ms\n",
      "Speed: 1.5ms preprocess, 52.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "166 406 406\n",
      "17\n",
      "10\n",
      "29\n",
      "167 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 47.7ms\n",
      "Speed: 1.2ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.7ms\n",
      "Speed: 1.7ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "168 406 406\n",
      "17\n",
      "5\n",
      "169 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 59.2ms\n",
      "Speed: 1.9ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "170 406 406\n",
      "10\n",
      "17\n",
      "171 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 70.3ms\n",
      "Speed: 1.3ms preprocess, 70.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 104.2ms\n",
      "Speed: 3.9ms preprocess, 104.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "172 406 406\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 47.3ms\n",
      "Speed: 1.2ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "173 406 406\n",
      "17\n",
      "10\n",
      "82\n",
      "174 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 57.1ms\n",
      "Speed: 1.2ms preprocess, 57.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 49.6ms\n",
      "Speed: 1.7ms preprocess, 49.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "82\n",
      "175 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 48.0ms\n",
      "Speed: 1.2ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "64\n",
      "176 406 406\n",
      "10\n",
      "17\n",
      "64\n",
      "177 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 55.4ms\n",
      "Speed: 1.2ms preprocess, 55.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.4ms\n",
      "Speed: 1.6ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64\n",
      "178 406 406\n",
      "10\n",
      "17\n",
      "179 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 48.2ms\n",
      "Speed: 1.4ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "82\n",
      "180 406 406\n",
      "17\n",
      "10\n",
      "82\n",
      "181 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 43.7ms\n",
      "Speed: 1.2ms preprocess, 43.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.5ms\n",
      "Speed: 1.2ms preprocess, 43.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 50.0ms\n",
      "Speed: 1.2ms preprocess, 50.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "182 406 406\n",
      "10\n",
      "17\n",
      "183 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 49.1ms\n",
      "Speed: 1.6ms preprocess, 49.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "184 406 406\n",
      "17\n",
      "10\n",
      "185 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.2ms\n",
      "Speed: 1.2ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "186 406 406\n",
      "10\n",
      "17\n",
      "187 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 44.3ms\n",
      "Speed: 1.3ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 46.6ms\n",
      "Speed: 1.1ms preprocess, 46.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "188 406 406\n",
      "10\n",
      "17\n",
      "189 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 46.9ms\n",
      "Speed: 1.2ms preprocess, 46.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64\n",
      "190 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.5ms\n",
      "Speed: 1.1ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "191 406 406\n",
      "10\n",
      "17\n",
      "64\n",
      "192 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 46.7ms\n",
      "Speed: 1.4ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 49.6ms\n",
      "Speed: 1.3ms preprocess, 49.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64\n",
      "193 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 44.1ms\n",
      "Speed: 1.4ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 52.6ms\n",
      "Speed: 1.1ms preprocess, 52.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "194 406 406\n",
      "10\n",
      "17\n",
      "64\n",
      "195 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 49.4ms\n",
      "Speed: 1.3ms preprocess, 49.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "64\n",
      "196 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "82\n",
      "197 406 406\n",
      "10\n",
      "17\n",
      "82\n",
      "198 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 1.2ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 43.8ms\n",
      "Speed: 1.2ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "82\n",
      "199 406 406\n",
      "34\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 44.9ms\n",
      "Speed: 1.2ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 46.0ms\n",
      "Speed: 1.7ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "200 406 406\n",
      "34\n",
      "17\n",
      "201 406 406\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 44.6ms\n",
      "Speed: 1.2ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "82\n",
      "202 406 406\n",
      "34\n",
      "17\n",
      "17\n",
      "203 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 59.7ms\n",
      "Speed: 1.1ms preprocess, 59.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 1.4ms preprocess, 43.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 48.1ms\n",
      "Speed: 1.2ms preprocess, 48.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "204 406 406\n",
      "10\n",
      "17\n",
      "205 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 45.9ms\n",
      "Speed: 1.2ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.1ms\n",
      "Speed: 1.2ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "206 406 406\n",
      "17\n",
      "10\n",
      "207 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.3ms\n",
      "Speed: 1.2ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "208 406 406\n",
      "34\n",
      "17\n",
      "209 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 42.6ms\n",
      "Speed: 1.2ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "210 406 406\n",
      "34\n",
      "17\n",
      "17\n",
      "211 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 42.3ms\n",
      "Speed: 1.0ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "212 406 406\n",
      "34\n",
      "17\n",
      "213 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 42.0ms\n",
      "Speed: 1.1ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "82\n",
      "214 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "215 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 40.7ms\n",
      "Speed: 1.2ms preprocess, 40.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 40.6ms\n",
      "Speed: 1.2ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "216 406 406\n",
      "10\n",
      "17\n",
      "217 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 41.6ms\n",
      "Speed: 1.1ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 1.2ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "218 406 406\n",
      "10\n",
      "17\n",
      "219 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.8ms\n",
      "Speed: 1.1ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "220 406 406\n",
      "10\n",
      "17\n",
      "221 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.5ms\n",
      "Speed: 1.3ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "222 406 406\n",
      "10\n",
      "17\n",
      "223 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "224 406 406\n",
      "10\n",
      "17\n",
      "225 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 43.6ms\n",
      "Speed: 1.1ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "17\n",
      "226 406 406\n",
      "10\n",
      "17\n",
      "227 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "228 406 406\n",
      "62\n",
      "17\n",
      "17\n",
      "229 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 47.0ms\n",
      "Speed: 1.1ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "46\n",
      "230 406 406\n",
      "62\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 42.7ms\n",
      "Speed: 1.1ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "231 406 406\n",
      "62\n",
      "17\n",
      "17\n",
      "232 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 41.7ms\n",
      "Speed: 1.2ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "62\n",
      "233 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "234 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 65.5ms\n",
      "Speed: 1.3ms preprocess, 65.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "17\n",
      "235 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 39.4ms\n",
      "Speed: 1.2ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "236 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "46\n",
      "237 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "17\n",
      "238 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "239 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "240 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "241 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 66.3ms\n",
      "Speed: 2.1ms preprocess, 66.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "17\n",
      "242 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 51.7ms\n",
      "Speed: 1.4ms preprocess, 51.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "243 406 406\n",
      "17\n",
      "36\n",
      "17\n",
      "244 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 45.2ms\n",
      "Speed: 1.3ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 50.2ms\n",
      "Speed: 1.8ms preprocess, 50.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "33\n",
      "17\n",
      "245 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 40.4ms\n",
      "Speed: 1.1ms preprocess, 40.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 45.3ms\n",
      "Speed: 1.2ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "246 406 406\n",
      "17\n",
      "33\n",
      "17\n",
      "247 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 44.2ms\n",
      "Speed: 1.1ms preprocess, 44.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "33\n",
      "17\n",
      "248 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 46.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "249 406 406\n",
      "17\n",
      "36\n",
      "17\n",
      "250 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.3ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "251 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "252 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "253 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "254 406 406\n",
      "17\n",
      "10\n",
      "17\n",
      "255 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "17\n",
      "256 406 406\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 60.7ms\n",
      "Speed: 1.5ms preprocess, 60.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "82\n",
      "257 406 406\n",
      "17\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 53.1ms\n",
      "Speed: 1.2ms preprocess, 53.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "258 406 406\n",
      "17\n",
      "10\n",
      "17\n",
      "259 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "17\n",
      "260 406 406\n",
      "17\n",
      "10\n",
      "20\n",
      "261 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 50.8ms\n",
      "Speed: 1.6ms preprocess, 50.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 47.5ms\n",
      "Speed: 1.1ms preprocess, 47.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "262 406 406\n",
      "17\n",
      "10\n",
      "64\n",
      "263 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 42.5ms\n",
      "Speed: 1.1ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "34\n",
      "20\n",
      "264 406 406\n",
      "17\n",
      "32\n",
      "51\n",
      "265 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 44.5ms\n",
      "Speed: 1.1ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 48.5ms\n",
      "Speed: 1.1ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "29\n",
      "51\n",
      "266 406 406\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 52.6ms\n",
      "Speed: 2.0ms preprocess, 52.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "267 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 47.5ms\n",
      "Speed: 1.8ms preprocess, 47.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "268 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "269 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 42.1ms\n",
      "Speed: 1.5ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 39.2ms\n",
      "Speed: 1.2ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "17\n",
      "17\n",
      "270 406 406\n",
      "10\n",
      "17\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "271 406 406\n",
      "29\n",
      "17\n",
      "17\n",
      "272 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 51.8ms\n",
      "Speed: 1.4ms preprocess, 51.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "17\n",
      "17\n",
      "273 406 406\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 51.1ms\n",
      "Speed: 1.1ms preprocess, 51.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "62\n",
      "274 406 406\n",
      "34\n",
      "17\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 47.2ms\n",
      "Speed: 1.1ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "275 406 406\n",
      "29\n",
      "17\n",
      "62\n",
      "276 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 42.3ms\n",
      "Speed: 1.2ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "17\n",
      "62\n",
      "277 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "278 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "279 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "46\n",
      "280 406 406\n",
      "17\n",
      "10\n",
      "29\n",
      "46\n",
      "281 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 46.0ms\n",
      "Speed: 1.1ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "29\n",
      "282 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.3ms\n",
      "Speed: 1.9ms preprocess, 45.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "283 406 406\n",
      "17\n",
      "10\n",
      "29\n",
      "284 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 42.5ms\n",
      "Speed: 1.2ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "29\n",
      "285 406 406\n",
      "17\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "286 406 406\n",
      "17\n",
      "17\n",
      "33\n",
      "287 406 406\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "288 406 406\n",
      "17\n",
      "33\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 persons, 45.3ms\n",
      "Speed: 1.2ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "289 406 406\n",
      "32\n",
      "10\n",
      "17\n",
      "51\n",
      "290 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 47.8ms\n",
      "Speed: 1.7ms preprocess, 47.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 45.7ms\n",
      "Speed: 1.1ms preprocess, 45.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "59\n",
      "17\n",
      "291 406 406\n",
      "17\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 40.1ms\n",
      "Speed: 1.4ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "292 406 406\n",
      "33\n",
      "33\n",
      "17\n",
      "293 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 48.0ms\n",
      "Speed: 2.1ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "29\n",
      "294 406 406\n",
      "17\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 56.7ms\n",
      "Speed: 1.8ms preprocess, 56.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "295 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "296 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 53.4ms\n",
      "Speed: 1.6ms preprocess, 53.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 46.1ms\n",
      "Speed: 1.2ms preprocess, 46.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "297 406 406\n",
      "17\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.4ms\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 51.2ms\n",
      "Speed: 1.2ms preprocess, 51.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "298 406 406\n",
      "10\n",
      "17\n",
      "299 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 88.0ms\n",
      "Speed: 2.5ms preprocess, 88.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "300 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 45.5ms\n",
      "Speed: 1.2ms preprocess, 45.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.9ms\n",
      "Speed: 1.2ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "301 406 406\n",
      "10\n",
      "17\n",
      "302 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 44.4ms\n",
      "Speed: 1.2ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 47.9ms\n",
      "Speed: 1.1ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "303 406 406\n",
      "10\n",
      "17\n",
      "304 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 46.1ms\n",
      "Speed: 1.2ms preprocess, 46.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "305 406 406\n",
      "10\n",
      "17\n",
      "306 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 45.5ms\n",
      "Speed: 1.1ms preprocess, 45.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.5ms\n",
      "Speed: 1.2ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "307 406 406\n",
      "10\n",
      "17\n",
      "308 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.6ms\n",
      "Speed: 1.2ms preprocess, 42.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.5ms\n",
      "Speed: 1.3ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "309 406 406\n",
      "10\n",
      "17\n",
      "310 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 56.8ms\n",
      "Speed: 1.8ms preprocess, 56.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 48.2ms\n",
      "Speed: 1.1ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "311 406 406\n",
      "10\n",
      "17\n",
      "312 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 46.2ms\n",
      "Speed: 1.1ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "313 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "314 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 43.2ms\n",
      "Speed: 1.2ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.3ms\n",
      "Speed: 1.2ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 45.6ms\n",
      "Speed: 1.2ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "315 406 406\n",
      "10\n",
      "17\n",
      "316 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "317 406 406\n",
      "10\n",
      "17\n",
      "318 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 47.5ms\n",
      "Speed: 1.2ms preprocess, 47.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "319 406 406\n",
      "10\n",
      "17\n",
      "320 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 44.9ms\n",
      "Speed: 1.5ms preprocess, 44.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.1ms\n",
      "Speed: 1.2ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.3ms\n",
      "Speed: 1.2ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "321 406 406\n",
      "10\n",
      "17\n",
      "322 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 51.6ms\n",
      "Speed: 2.3ms preprocess, 51.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "17\n",
      "323 406 406\n",
      "10\n",
      "17\n",
      "324 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 47.7ms\n",
      "Speed: 1.5ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 56.5ms\n",
      "Speed: 1.2ms preprocess, 56.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "325 406 406\n",
      "10\n",
      "17\n",
      "326 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 54.3ms\n",
      "Speed: 1.2ms preprocess, 54.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 57.1ms\n",
      "Speed: 1.3ms preprocess, 57.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "62\n",
      "327 406 406\n",
      "33\n",
      "33\n",
      "328 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 51.5ms\n",
      "Speed: 1.9ms preprocess, 51.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.4ms\n",
      "Speed: 1.2ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 44.1ms\n",
      "Speed: 1.2ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33\n",
      "329 406 406\n",
      "33\n",
      "17\n",
      "330 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 47.1ms\n",
      "Speed: 1.2ms preprocess, 47.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "331 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "332 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 1.3ms preprocess, 42.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 50.9ms\n",
      "Speed: 1.8ms preprocess, 50.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.9ms\n",
      "Speed: 1.1ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "333 406 406\n",
      "10\n",
      "33\n",
      "334 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.8ms\n",
      "Speed: 1.2ms preprocess, 42.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.6ms\n",
      "Speed: 1.2ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "335 406 406\n",
      "10\n",
      "17\n",
      "336 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 47.8ms\n",
      "Speed: 1.3ms preprocess, 47.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "337 406 406\n",
      "10\n",
      "17\n",
      "338 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.7ms\n",
      "Speed: 1.2ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "339 406 406\n",
      "10\n",
      "33\n",
      "340 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 42.8ms\n",
      "Speed: 1.1ms preprocess, 42.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "341 406 406\n",
      "10\n",
      "33\n",
      "342 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 42.6ms\n",
      "Speed: 1.2ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.6ms\n",
      "Speed: 1.1ms preprocess, 41.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "343 406 406\n",
      "10\n",
      "17\n",
      "344 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 41.9ms\n",
      "Speed: 1.2ms preprocess, 41.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "33\n",
      "345 406 406\n",
      "10\n",
      "33\n",
      "346 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "347 406 406\n",
      "10\n",
      "33\n",
      "348 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "349 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "350 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 46.6ms\n",
      "Speed: 1.3ms preprocess, 46.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "351 406 406\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 45.8ms\n",
      "Speed: 1.1ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "352 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "353 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "354 406 406\n",
      "10\n",
      "17\n",
      "355 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "356 406 406\n",
      "10\n",
      "17\n",
      "357 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "358 406 406\n",
      "10\n",
      "17\n",
      "359 406 406\n",
      "10\n",
      "17\n",
      "360 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 42.0ms\n",
      "Speed: 1.1ms preprocess, 42.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "361 406 406\n",
      "10\n",
      "17\n",
      "362 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 40.1ms\n",
      "Speed: 1.2ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "363 406 406\n",
      "10\n",
      "76\n",
      "364 406 406\n",
      "10\n",
      "29\n",
      "365 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "76\n",
      "17\n",
      "366 406 406\n",
      "10\n",
      "29\n",
      "17\n",
      "367 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 58.1ms\n",
      "Speed: 1.9ms preprocess, 58.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "368 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "369 406 406\n",
      "10\n",
      "33\n",
      "17\n",
      "370 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "371 406 406\n",
      "10\n",
      "29\n",
      "372 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "373 406 406\n",
      "10\n",
      "29\n",
      "17\n",
      "374 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "95\n",
      "17\n",
      "375 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "376 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "377 406 406\n",
      "10\n",
      "17\n",
      "378 406 406\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "95\n",
      "379 406 406\n",
      "10\n",
      "17\n",
      "33\n",
      "380 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 41.9ms\n",
      "Speed: 1.0ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "64\n",
      "33\n",
      "381 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "382 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "33\n",
      "383 406 406\n",
      "64\n",
      "10\n",
      "10\n",
      "384 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.7ms\n",
      "Speed: 1.2ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "10\n",
      "62\n",
      "385 406 406\n",
      "17\n",
      "10\n",
      "17\n",
      "386 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "10\n",
      "17\n",
      "387 406 406\n",
      "76\n",
      "10\n",
      "17\n",
      "388 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 1.2ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "389 406 406\n",
      "17\n",
      "17\n",
      "10\n",
      "390 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "10\n",
      "17\n",
      "391 406 406\n",
      "17\n",
      "17\n",
      "10\n",
      "392 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "393 406 406\n",
      "64\n",
      "10\n",
      "17\n",
      "394 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "10\n",
      "31\n",
      "395 406 406\n",
      "10\n",
      "64\n",
      "65\n",
      "396 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "64\n",
      "65\n",
      "397 406 406\n",
      "10\n",
      "64\n",
      "17\n",
      "398 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "399 406 406\n",
      "10\n",
      "64\n",
      "17\n",
      "400 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "17\n",
      "401 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "402 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "17\n",
      "29\n",
      "403 406 406\n",
      "10\n",
      "17\n",
      "17\n",
      "404 406 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "29\n",
      "17\n",
      "405 406 406\n",
      "29\n",
      "10\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_video_path = 'zgmFz3vDU-HhuTJh7409Jg.mp4'\n",
    "output_json_path = os.path.join(\"result\", input_video_path.replace(\".mp4\", \"_result.json\"))\n",
    "actors_priority = 'actors_lists.json'\n",
    "video_metadata = 'zgmFz3vDU-HhuTJh7409Jg.json'\n",
    "\n",
    "try:\n",
    "    with open(actors_priority, \"r\") as file:\n",
    "        actors_priority = json.load(file)\n",
    "except:\n",
    "    actors_priority == None\n",
    "\n",
    "try:\n",
    "    with open(video_metadata, \"r\") as file:\n",
    "        video_metadata = json.load(file)\n",
    "except:\n",
    "    video_metadata = None\n",
    "final_output = process_video(input_video_path, output_json_path, actors_priority, video_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'frame_number': 0, 'crop_coordinates': 102.30856323242188},\n",
       " {'frame_number': 1, 'crop_coordinates': 101.1241455078125},\n",
       " {'frame_number': 2, 'crop_coordinates': 101.39639282226562},\n",
       " {'frame_number': 3, 'crop_coordinates': 100.60745239257812},\n",
       " {'frame_number': 4, 'crop_coordinates': 100.56619262695312},\n",
       " {'frame_number': 5, 'crop_coordinates': 100.65182495117188},\n",
       " {'frame_number': 6, 'crop_coordinates': 100.52545166015625},\n",
       " {'frame_number': 7, 'crop_coordinates': 100.29180908203125},\n",
       " {'frame_number': 8, 'crop_coordinates': 99.35391235351562},\n",
       " {'frame_number': 9, 'crop_coordinates': 99.2406005859375},\n",
       " {'frame_number': 10, 'crop_coordinates': 124.13174438476562},\n",
       " {'frame_number': 11, 'crop_coordinates': 99.33193969726562},\n",
       " {'frame_number': 12, 'crop_coordinates': 100.34378051757812},\n",
       " {'frame_number': 13, 'crop_coordinates': 100.66732788085938},\n",
       " {'frame_number': 14, 'crop_coordinates': 100.8067626953125},\n",
       " {'frame_number': 15, 'crop_coordinates': 100.87530517578125},\n",
       " {'frame_number': 16, 'crop_coordinates': 101.47732543945312},\n",
       " {'frame_number': 17, 'crop_coordinates': 100.85214233398438},\n",
       " {'frame_number': 18, 'crop_coordinates': 100.873779296875},\n",
       " {'frame_number': 19, 'crop_coordinates': 100.16561889648438},\n",
       " {'frame_number': 20, 'crop_coordinates': 100.202880859375},\n",
       " {'frame_number': 21, 'crop_coordinates': 100.39715576171875},\n",
       " {'frame_number': 22, 'crop_coordinates': 100.56118774414062},\n",
       " {'frame_number': 23, 'crop_coordinates': 100.60870361328125},\n",
       " {'frame_number': 24, 'crop_coordinates': 100.6690673828125},\n",
       " {'frame_number': 25, 'crop_coordinates': 100.51583862304688},\n",
       " {'frame_number': 26, 'crop_coordinates': 99.66058349609375},\n",
       " {'frame_number': 27, 'crop_coordinates': 99.86361694335938},\n",
       " {'frame_number': 28, 'crop_coordinates': 99.03274536132812},\n",
       " {'frame_number': 29, 'crop_coordinates': 98.96710205078125},\n",
       " {'frame_number': 30, 'crop_coordinates': 98.88983154296875},\n",
       " {'frame_number': 31, 'crop_coordinates': 123.0567626953125},\n",
       " {'frame_number': 32, 'crop_coordinates': 123.09130859375},\n",
       " {'frame_number': 33, 'crop_coordinates': 122.13058471679688},\n",
       " {'frame_number': 34, 'crop_coordinates': 121.85702514648438},\n",
       " {'frame_number': 35, 'crop_coordinates': 121.776123046875},\n",
       " {'frame_number': 36, 'crop_coordinates': 100.69671630859375},\n",
       " {'frame_number': 37, 'crop_coordinates': 121.1856689453125},\n",
       " {'frame_number': 38, 'crop_coordinates': 100.7747802734375},\n",
       " {'frame_number': 39, 'crop_coordinates': 100.87606811523438},\n",
       " {'frame_number': 40, 'crop_coordinates': 100.96987915039062},\n",
       " {'frame_number': 41, 'crop_coordinates': 101.45697021484375},\n",
       " {'frame_number': 42, 'crop_coordinates': 101.47119140625},\n",
       " {'frame_number': 43, 'crop_coordinates': 101.65689086914062},\n",
       " {'frame_number': 44, 'crop_coordinates': 101.91818237304688},\n",
       " {'frame_number': 45, 'crop_coordinates': 102.0042724609375},\n",
       " {'frame_number': 46, 'crop_coordinates': 102.17440795898438},\n",
       " {'frame_number': 47, 'crop_coordinates': 103.35711669921875},\n",
       " {'frame_number': 48, 'crop_coordinates': 103.22781372070312},\n",
       " {'frame_number': 49, 'crop_coordinates': 103.02725219726562},\n",
       " {'frame_number': 50, 'crop_coordinates': 119.49383544921875},\n",
       " {'frame_number': 51, 'crop_coordinates': 119.40359497070312},\n",
       " {'frame_number': 52, 'crop_coordinates': 119.9119873046875},\n",
       " {'frame_number': 53, 'crop_coordinates': 118.84707641601562},\n",
       " {'frame_number': 54, 'crop_coordinates': 103.01144409179688},\n",
       " {'frame_number': 55, 'crop_coordinates': 118.46304321289062},\n",
       " {'frame_number': 56, 'crop_coordinates': 103.077880859375},\n",
       " {'frame_number': 57, 'crop_coordinates': 102.32098388671875},\n",
       " {'frame_number': 58, 'crop_coordinates': 102.19418334960938},\n",
       " {'frame_number': 59, 'crop_coordinates': 102.01898193359375},\n",
       " {'frame_number': 60, 'crop_coordinates': 115.18905639648438},\n",
       " {'frame_number': 61, 'crop_coordinates': 100.97402954101562},\n",
       " {'frame_number': 62, 'crop_coordinates': 100.99490356445312},\n",
       " {'frame_number': 63, 'crop_coordinates': 101.13101196289062},\n",
       " {'frame_number': 64, 'crop_coordinates': 101.58782958984375},\n",
       " {'frame_number': 65, 'crop_coordinates': 101.4881591796875},\n",
       " {'frame_number': 66, 'crop_coordinates': 101.59136962890625},\n",
       " {'frame_number': 67, 'crop_coordinates': 101.951171875},\n",
       " {'frame_number': 68, 'crop_coordinates': 102.13607788085938},\n",
       " {'frame_number': 69, 'crop_coordinates': 101.81002807617188},\n",
       " {'frame_number': 70, 'crop_coordinates': 102.58282470703125},\n",
       " {'frame_number': 71, 'crop_coordinates': 102.22885131835938},\n",
       " {'frame_number': 72, 'crop_coordinates': 102.37249755859375},\n",
       " {'frame_number': 73, 'crop_coordinates': 102.23812866210938},\n",
       " {'frame_number': 74, 'crop_coordinates': 101.201171875},\n",
       " {'frame_number': 75, 'crop_coordinates': 101.24066162109375},\n",
       " {'frame_number': 76, 'crop_coordinates': 101.23062133789062},\n",
       " {'frame_number': 77, 'crop_coordinates': 101.15151977539062},\n",
       " {'frame_number': 78, 'crop_coordinates': 101.10867309570312},\n",
       " {'frame_number': 79, 'crop_coordinates': 101.10354614257812},\n",
       " {'frame_number': 80, 'crop_coordinates': 101.1187744140625},\n",
       " {'frame_number': 81, 'crop_coordinates': 101.072265625},\n",
       " {'frame_number': 82, 'crop_coordinates': 101.9779052734375},\n",
       " {'frame_number': 83, 'crop_coordinates': 101.27236938476562},\n",
       " {'frame_number': 84, 'crop_coordinates': 101.49609375},\n",
       " {'frame_number': 85, 'crop_coordinates': 101.39453125},\n",
       " {'frame_number': 86, 'crop_coordinates': 100.35113525390625},\n",
       " {'frame_number': 87, 'crop_coordinates': 101.01803588867188},\n",
       " {'frame_number': 88, 'crop_coordinates': 99.95635986328125},\n",
       " {'frame_number': 89, 'crop_coordinates': 100.51046752929688},\n",
       " {'frame_number': 90, 'crop_coordinates': 99.54733276367188},\n",
       " {'frame_number': 91, 'crop_coordinates': 99.89959716796875},\n",
       " {'frame_number': 92, 'crop_coordinates': 99.82040405273438},\n",
       " {'frame_number': 93, 'crop_coordinates': 120.5703125},\n",
       " {'frame_number': 94, 'crop_coordinates': 100.26922607421875},\n",
       " {'frame_number': 95, 'crop_coordinates': 101.77947998046875},\n",
       " {'frame_number': 96, 'crop_coordinates': 102.72976684570312},\n",
       " {'frame_number': 97, 'crop_coordinates': 88.10464477539062},\n",
       " {'frame_number': 98, 'crop_coordinates': 117.80795288085938},\n",
       " {'frame_number': 99, 'crop_coordinates': 116.67434692382812},\n",
       " {'frame_number': 100, 'crop_coordinates': 104.5438232421875},\n",
       " {'frame_number': 101, 'crop_coordinates': 86.55242919921875},\n",
       " {'frame_number': 102, 'crop_coordinates': 105.96759033203125},\n",
       " {'frame_number': 103, 'crop_coordinates': 105.8612060546875},\n",
       " {'frame_number': 104, 'crop_coordinates': 106.10354614257812},\n",
       " {'frame_number': 105, 'crop_coordinates': 105.87188720703125},\n",
       " {'frame_number': 106, 'crop_coordinates': 105.36410522460938},\n",
       " {'frame_number': 107, 'crop_coordinates': 105.18267822265625},\n",
       " {'frame_number': 108, 'crop_coordinates': 105.60165405273438},\n",
       " {'frame_number': 109, 'crop_coordinates': 105.80270385742188},\n",
       " {'frame_number': 110, 'crop_coordinates': 106.39007568359375},\n",
       " {'frame_number': 111, 'crop_coordinates': 106.03350830078125},\n",
       " {'frame_number': 112, 'crop_coordinates': 106.35357666015625},\n",
       " {'frame_number': 113, 'crop_coordinates': 85.93325805664062},\n",
       " {'frame_number': 114, 'crop_coordinates': 107.008056640625},\n",
       " {'frame_number': 115, 'crop_coordinates': 107.21365356445312},\n",
       " {'frame_number': 116, 'crop_coordinates': 87.45480346679688},\n",
       " {'frame_number': 117, 'crop_coordinates': 87.36602783203125},\n",
       " {'frame_number': 118, 'crop_coordinates': 115.54086303710938},\n",
       " {'frame_number': 119, 'crop_coordinates': 116.22244262695312},\n",
       " {'frame_number': 120, 'crop_coordinates': 116.01904296875},\n",
       " {'frame_number': 121, 'crop_coordinates': 104.88751220703125},\n",
       " {'frame_number': 122, 'crop_coordinates': 104.546630859375},\n",
       " {'frame_number': 123, 'crop_coordinates': 105.30197143554688},\n",
       " {'frame_number': 124, 'crop_coordinates': 105.1900634765625},\n",
       " {'frame_number': 125, 'crop_coordinates': 106.25982666015625},\n",
       " {'frame_number': 126, 'crop_coordinates': 106.75},\n",
       " {'frame_number': 127, 'crop_coordinates': 105.6990966796875},\n",
       " {'frame_number': 128, 'crop_coordinates': 105.07183837890625},\n",
       " {'frame_number': 129, 'crop_coordinates': 104.51141357421875},\n",
       " {'frame_number': 130, 'crop_coordinates': 105.4969482421875},\n",
       " {'frame_number': 131, 'crop_coordinates': 105.75},\n",
       " {'frame_number': 132, 'crop_coordinates': 105.75},\n",
       " {'frame_number': 133, 'crop_coordinates': 105.40139770507812},\n",
       " {'frame_number': 134, 'crop_coordinates': 105.49078369140625},\n",
       " {'frame_number': 135, 'crop_coordinates': 105.54559326171875},\n",
       " {'frame_number': 136, 'crop_coordinates': 105.57925415039062},\n",
       " {'frame_number': 137, 'crop_coordinates': 105.69863891601562},\n",
       " {'frame_number': 138, 'crop_coordinates': 105.75515747070312},\n",
       " {'frame_number': 139, 'crop_coordinates': 106.73379516601562},\n",
       " {'frame_number': 140, 'crop_coordinates': 105.630126953125},\n",
       " {'frame_number': 141, 'crop_coordinates': 105.83853149414062},\n",
       " {'frame_number': 142, 'crop_coordinates': 106.77053833007812},\n",
       " {'frame_number': 143, 'crop_coordinates': 106.65902709960938},\n",
       " {'frame_number': 144, 'crop_coordinates': 105.63592529296875},\n",
       " {'frame_number': 145, 'crop_coordinates': 115.49725341796875},\n",
       " {'frame_number': 146, 'crop_coordinates': 115.3267822265625},\n",
       " {'frame_number': 147, 'crop_coordinates': 105.15390014648438},\n",
       " {'frame_number': 148, 'crop_coordinates': 103.42684936523438},\n",
       " {'frame_number': 149, 'crop_coordinates': 115.4853515625},\n",
       " {'frame_number': 150, 'crop_coordinates': 115.52352905273438},\n",
       " {'frame_number': 151, 'crop_coordinates': 115.6904296875},\n",
       " {'frame_number': 152, 'crop_coordinates': 115.71450805664062},\n",
       " {'frame_number': 153, 'crop_coordinates': 115.7745361328125},\n",
       " {'frame_number': 154, 'crop_coordinates': 99.61502075195312},\n",
       " {'frame_number': 155, 'crop_coordinates': 99.548583984375},\n",
       " {'frame_number': 156, 'crop_coordinates': 114.90237426757812},\n",
       " {'frame_number': 157, 'crop_coordinates': 114.84588623046875},\n",
       " {'frame_number': 158, 'crop_coordinates': 100.13882446289062},\n",
       " {'frame_number': 159, 'crop_coordinates': 100.30972290039062},\n",
       " {'frame_number': 160, 'crop_coordinates': 114.99014282226562},\n",
       " {'frame_number': 161, 'crop_coordinates': 99.31173706054688},\n",
       " {'frame_number': 162, 'crop_coordinates': 99.29034423828125},\n",
       " {'frame_number': 163, 'crop_coordinates': 99.36370849609375},\n",
       " {'frame_number': 164, 'crop_coordinates': 115.48922729492188},\n",
       " {'frame_number': 165, 'crop_coordinates': 115.52783203125},\n",
       " {'frame_number': 166, 'crop_coordinates': 115.48138427734375},\n",
       " {'frame_number': 167, 'crop_coordinates': 115.70123291015625},\n",
       " {'frame_number': 168, 'crop_coordinates': 115.3209228515625},\n",
       " {'frame_number': 169, 'crop_coordinates': 99.14700317382812},\n",
       " {'frame_number': 170, 'crop_coordinates': 99.11196899414062},\n",
       " {'frame_number': 171, 'crop_coordinates': 114.867919921875},\n",
       " {'frame_number': 172, 'crop_coordinates': 114.85699462890625},\n",
       " {'frame_number': 173, 'crop_coordinates': 114.92279052734375},\n",
       " {'frame_number': 174, 'crop_coordinates': 114.99142456054688},\n",
       " {'frame_number': 175, 'crop_coordinates': 100.308349609375},\n",
       " {'frame_number': 176, 'crop_coordinates': 100.1651611328125},\n",
       " {'frame_number': 177, 'crop_coordinates': 100.14761352539062},\n",
       " {'frame_number': 178, 'crop_coordinates': 100.22317504882812},\n",
       " {'frame_number': 179, 'crop_coordinates': 100.18215942382812},\n",
       " {'frame_number': 180, 'crop_coordinates': 115.66085815429688},\n",
       " {'frame_number': 181, 'crop_coordinates': 114.97531127929688},\n",
       " {'frame_number': 182, 'crop_coordinates': 100.24566650390625},\n",
       " {'frame_number': 183, 'crop_coordinates': 114.89633178710938},\n",
       " {'frame_number': 184, 'crop_coordinates': 114.7884521484375},\n",
       " {'frame_number': 185, 'crop_coordinates': 115.74755859375},\n",
       " {'frame_number': 186, 'crop_coordinates': 100.34091186523438},\n",
       " {'frame_number': 187, 'crop_coordinates': 100.06527709960938},\n",
       " {'frame_number': 188, 'crop_coordinates': 101.26962280273438},\n",
       " {'frame_number': 189, 'crop_coordinates': 101.35189819335938},\n",
       " {'frame_number': 190, 'crop_coordinates': 100.81808471679688},\n",
       " {'frame_number': 191, 'crop_coordinates': 101.0501708984375},\n",
       " {'frame_number': 192, 'crop_coordinates': 102.19375610351562},\n",
       " {'frame_number': 193, 'crop_coordinates': 102.15744018554688},\n",
       " {'frame_number': 194, 'crop_coordinates': 102.2215576171875},\n",
       " {'frame_number': 195, 'crop_coordinates': 102.38787841796875},\n",
       " {'frame_number': 196, 'crop_coordinates': 101.49801635742188},\n",
       " {'frame_number': 197, 'crop_coordinates': 100.48614501953125},\n",
       " {'frame_number': 198, 'crop_coordinates': 100.31082153320312},\n",
       " {'frame_number': 199, 'crop_coordinates': 100.4033203125},\n",
       " {'frame_number': 200, 'crop_coordinates': 100.39743041992188},\n",
       " {'frame_number': 201, 'crop_coordinates': 99.87057495117188},\n",
       " {'frame_number': 202, 'crop_coordinates': 99.412109375},\n",
       " {'frame_number': 203, 'crop_coordinates': 115.40850830078125},\n",
       " {'frame_number': 204, 'crop_coordinates': 99.44253540039062},\n",
       " {'frame_number': 205, 'crop_coordinates': 100.21347045898438},\n",
       " {'frame_number': 206, 'crop_coordinates': 115.26852416992188},\n",
       " {'frame_number': 207, 'crop_coordinates': 100.90960693359375},\n",
       " {'frame_number': 208, 'crop_coordinates': 100.99594116210938},\n",
       " {'frame_number': 209, 'crop_coordinates': 101.51803588867188},\n",
       " {'frame_number': 210, 'crop_coordinates': 100.9005126953125},\n",
       " {'frame_number': 211, 'crop_coordinates': 101.33551025390625},\n",
       " {'frame_number': 212, 'crop_coordinates': 100.23992919921875},\n",
       " {'frame_number': 213, 'crop_coordinates': 100.27093505859375},\n",
       " {'frame_number': 214, 'crop_coordinates': 99.06063842773438},\n",
       " {'frame_number': 215, 'crop_coordinates': 98.81765747070312},\n",
       " {'frame_number': 216, 'crop_coordinates': 98.75704956054688},\n",
       " {'frame_number': 217, 'crop_coordinates': 99.74301147460938},\n",
       " {'frame_number': 218, 'crop_coordinates': 99.8658447265625},\n",
       " {'frame_number': 219, 'crop_coordinates': 98.13861083984375},\n",
       " {'frame_number': 220, 'crop_coordinates': 98.16238403320312},\n",
       " {'frame_number': 221, 'crop_coordinates': 99.07366943359375},\n",
       " {'frame_number': 222, 'crop_coordinates': 98.88668823242188},\n",
       " {'frame_number': 223, 'crop_coordinates': 99.6395263671875},\n",
       " {'frame_number': 224, 'crop_coordinates': 99.56915283203125},\n",
       " {'frame_number': 225, 'crop_coordinates': 98.80313110351562},\n",
       " {'frame_number': 226, 'crop_coordinates': 99.753173828125},\n",
       " {'frame_number': 227, 'crop_coordinates': 99.74429321289062},\n",
       " {'frame_number': 228, 'crop_coordinates': 98.85348510742188},\n",
       " {'frame_number': 229, 'crop_coordinates': 98.7257080078125},\n",
       " {'frame_number': 230, 'crop_coordinates': 98.69287109375},\n",
       " {'frame_number': 231, 'crop_coordinates': 98.75955200195312},\n",
       " {'frame_number': 232, 'crop_coordinates': 98.82046508789062},\n",
       " {'frame_number': 233, 'crop_coordinates': 98.75515747070312},\n",
       " {'frame_number': 234, 'crop_coordinates': 98.77548217773438},\n",
       " {'frame_number': 235, 'crop_coordinates': 98.76785278320312},\n",
       " {'frame_number': 236, 'crop_coordinates': 99.71353149414062},\n",
       " {'frame_number': 237, 'crop_coordinates': 99.6441650390625},\n",
       " {'frame_number': 238, 'crop_coordinates': 100.92495727539062},\n",
       " {'frame_number': 239, 'crop_coordinates': 100.46051025390625},\n",
       " {'frame_number': 240, 'crop_coordinates': 101.3160400390625},\n",
       " {'frame_number': 241, 'crop_coordinates': 116.83175659179688},\n",
       " {'frame_number': 242, 'crop_coordinates': 100.19039916992188},\n",
       " {'frame_number': 243, 'crop_coordinates': 117.00146484375},\n",
       " {'frame_number': 244, 'crop_coordinates': 118.884521484375},\n",
       " {'frame_number': 245, 'crop_coordinates': 117.03732299804688},\n",
       " {'frame_number': 246, 'crop_coordinates': 118.05630493164062},\n",
       " {'frame_number': 247, 'crop_coordinates': 118.65780639648438},\n",
       " {'frame_number': 248, 'crop_coordinates': 118.0709228515625},\n",
       " {'frame_number': 249, 'crop_coordinates': 117.91128540039062},\n",
       " {'frame_number': 250, 'crop_coordinates': 100.23681640625},\n",
       " {'frame_number': 251, 'crop_coordinates': 98.37130737304688},\n",
       " {'frame_number': 252, 'crop_coordinates': 98.346435546875},\n",
       " {'frame_number': 253, 'crop_coordinates': 98.28228759765625},\n",
       " {'frame_number': 254, 'crop_coordinates': 118.659912109375},\n",
       " {'frame_number': 255, 'crop_coordinates': 118.38406372070312},\n",
       " {'frame_number': 256, 'crop_coordinates': 120.19219970703125},\n",
       " {'frame_number': 257, 'crop_coordinates': 119.6785888671875},\n",
       " {'frame_number': 258, 'crop_coordinates': 119.06024169921875},\n",
       " {'frame_number': 259, 'crop_coordinates': 119.048828125},\n",
       " {'frame_number': 260, 'crop_coordinates': 119.13525390625},\n",
       " {'frame_number': 261, 'crop_coordinates': 119.36541748046875},\n",
       " {'frame_number': 262, 'crop_coordinates': 117.19049072265625},\n",
       " {'frame_number': 263, 'crop_coordinates': 117.8795166015625},\n",
       " {'frame_number': 264, 'crop_coordinates': 117.90902709960938},\n",
       " {'frame_number': 265, 'crop_coordinates': 116.86898803710938},\n",
       " {'frame_number': 266, 'crop_coordinates': 116.84259033203125},\n",
       " {'frame_number': 267, 'crop_coordinates': 116.84963989257812},\n",
       " {'frame_number': 268, 'crop_coordinates': 99.31594848632812},\n",
       " {'frame_number': 269, 'crop_coordinates': 99.42129516601562},\n",
       " {'frame_number': 270, 'crop_coordinates': 99.63720703125},\n",
       " {'frame_number': 271, 'crop_coordinates': 99.58880615234375},\n",
       " {'frame_number': 272, 'crop_coordinates': 101.46133422851562},\n",
       " {'frame_number': 273, 'crop_coordinates': 101.3228759765625},\n",
       " {'frame_number': 274, 'crop_coordinates': 99.682861328125},\n",
       " {'frame_number': 275, 'crop_coordinates': 101.36233520507812},\n",
       " {'frame_number': 276, 'crop_coordinates': 100.31082153320312},\n",
       " {'frame_number': 277, 'crop_coordinates': 98.22604370117188},\n",
       " {'frame_number': 278, 'crop_coordinates': 98.39688110351562},\n",
       " {'frame_number': 279, 'crop_coordinates': 98.814208984375},\n",
       " {'frame_number': 280, 'crop_coordinates': 117.921630859375},\n",
       " {'frame_number': 281, 'crop_coordinates': 118.48208618164062},\n",
       " {'frame_number': 282, 'crop_coordinates': 117.94070434570312},\n",
       " {'frame_number': 283, 'crop_coordinates': 117.20596313476562},\n",
       " {'frame_number': 284, 'crop_coordinates': 118.66220092773438},\n",
       " {'frame_number': 285, 'crop_coordinates': 117.2822265625},\n",
       " {'frame_number': 286, 'crop_coordinates': 118.18472290039062},\n",
       " {'frame_number': 287, 'crop_coordinates': 118.17010498046875},\n",
       " {'frame_number': 288, 'crop_coordinates': 116.44900512695312},\n",
       " {'frame_number': 289, 'crop_coordinates': 118.5230712890625},\n",
       " {'frame_number': 290, 'crop_coordinates': 117.95297241210938},\n",
       " {'frame_number': 291, 'crop_coordinates': 116.82174682617188},\n",
       " {'frame_number': 292, 'crop_coordinates': 100.15420532226562},\n",
       " {'frame_number': 293, 'crop_coordinates': 100.08740234375},\n",
       " {'frame_number': 294, 'crop_coordinates': 118.66049194335938},\n",
       " {'frame_number': 295, 'crop_coordinates': 99.95391845703125},\n",
       " {'frame_number': 296, 'crop_coordinates': 99.94595336914062},\n",
       " {'frame_number': 297, 'crop_coordinates': 117.83184814453125},\n",
       " {'frame_number': 298, 'crop_coordinates': 100.26123046875},\n",
       " {'frame_number': 299, 'crop_coordinates': 100.38134765625},\n",
       " {'frame_number': 300, 'crop_coordinates': 101.37863159179688},\n",
       " {'frame_number': 301, 'crop_coordinates': 101.24154663085938},\n",
       " {'frame_number': 302, 'crop_coordinates': 101.42391967773438},\n",
       " {'frame_number': 303, 'crop_coordinates': 101.92315673828125},\n",
       " {'frame_number': 304, 'crop_coordinates': 99.625},\n",
       " {'frame_number': 305, 'crop_coordinates': 99.64828491210938},\n",
       " {'frame_number': 306, 'crop_coordinates': 99.71536254882812},\n",
       " {'frame_number': 307, 'crop_coordinates': 99.9608154296875},\n",
       " {'frame_number': 308, 'crop_coordinates': 100.0167236328125},\n",
       " {'frame_number': 309, 'crop_coordinates': 99.87860107421875},\n",
       " {'frame_number': 310, 'crop_coordinates': 99.90789794921875},\n",
       " {'frame_number': 311, 'crop_coordinates': 99.87448120117188},\n",
       " {'frame_number': 312, 'crop_coordinates': 100.78775024414062},\n",
       " {'frame_number': 313, 'crop_coordinates': 99.90377807617188},\n",
       " {'frame_number': 314, 'crop_coordinates': 100.76446533203125},\n",
       " {'frame_number': 315, 'crop_coordinates': 100.77120971679688},\n",
       " {'frame_number': 316, 'crop_coordinates': 100.75341796875},\n",
       " {'frame_number': 317, 'crop_coordinates': 100.8692626953125},\n",
       " {'frame_number': 318, 'crop_coordinates': 99.83551025390625},\n",
       " {'frame_number': 319, 'crop_coordinates': 99.793212890625},\n",
       " {'frame_number': 320, 'crop_coordinates': 99.86203002929688},\n",
       " {'frame_number': 321, 'crop_coordinates': 99.77572631835938},\n",
       " {'frame_number': 322, 'crop_coordinates': 99.87445068359375},\n",
       " {'frame_number': 323, 'crop_coordinates': 99.94943237304688},\n",
       " {'frame_number': 324, 'crop_coordinates': 99.9793701171875},\n",
       " {'frame_number': 325, 'crop_coordinates': 100.07717895507812},\n",
       " {'frame_number': 326, 'crop_coordinates': 100.01736450195312},\n",
       " {'frame_number': 327, 'crop_coordinates': 100.19952392578125},\n",
       " {'frame_number': 328, 'crop_coordinates': 100.14703369140625},\n",
       " {'frame_number': 329, 'crop_coordinates': 100.18692016601562},\n",
       " {'frame_number': 330, 'crop_coordinates': 100.07470703125},\n",
       " {'frame_number': 331, 'crop_coordinates': 100.04989624023438},\n",
       " {'frame_number': 332, 'crop_coordinates': 100.8905029296875},\n",
       " {'frame_number': 333, 'crop_coordinates': 99.93695068359375},\n",
       " {'frame_number': 334, 'crop_coordinates': 99.8665771484375},\n",
       " {'frame_number': 335, 'crop_coordinates': 99.8165283203125},\n",
       " {'frame_number': 336, 'crop_coordinates': 99.80984497070312},\n",
       " {'frame_number': 337, 'crop_coordinates': 99.8934326171875},\n",
       " {'frame_number': 338, 'crop_coordinates': 99.895263671875},\n",
       " {'frame_number': 339, 'crop_coordinates': 99.92733764648438},\n",
       " {'frame_number': 340, 'crop_coordinates': 100.02899169921875},\n",
       " {'frame_number': 341, 'crop_coordinates': 100.12432861328125},\n",
       " {'frame_number': 342, 'crop_coordinates': 100.06301879882812},\n",
       " {'frame_number': 343, 'crop_coordinates': 100.03176879882812},\n",
       " {'frame_number': 344, 'crop_coordinates': 100.07534790039062},\n",
       " {'frame_number': 345, 'crop_coordinates': 100.18399047851562},\n",
       " {'frame_number': 346, 'crop_coordinates': 100.12762451171875},\n",
       " {'frame_number': 347, 'crop_coordinates': 100.058837890625},\n",
       " {'frame_number': 348, 'crop_coordinates': 100.0772705078125},\n",
       " {'frame_number': 349, 'crop_coordinates': 100.02294921875},\n",
       " {'frame_number': 350, 'crop_coordinates': 100.0843505859375},\n",
       " {'frame_number': 351, 'crop_coordinates': 100.03515625},\n",
       " {'frame_number': 352, 'crop_coordinates': 100.11410522460938},\n",
       " {'frame_number': 353, 'crop_coordinates': 100.0362548828125},\n",
       " {'frame_number': 354, 'crop_coordinates': 99.84609985351562},\n",
       " {'frame_number': 355, 'crop_coordinates': 99.76077270507812},\n",
       " {'frame_number': 356, 'crop_coordinates': 99.64498901367188},\n",
       " {'frame_number': 357, 'crop_coordinates': 99.73947143554688},\n",
       " {'frame_number': 358, 'crop_coordinates': 99.77236938476562},\n",
       " {'frame_number': 359, 'crop_coordinates': 100.52987670898438},\n",
       " {'frame_number': 360, 'crop_coordinates': 99.78292846679688},\n",
       " {'frame_number': 361, 'crop_coordinates': 99.75808715820312},\n",
       " {'frame_number': 362, 'crop_coordinates': 100.64874267578125},\n",
       " {'frame_number': 363, 'crop_coordinates': 99.74575805664062},\n",
       " {'frame_number': 364, 'crop_coordinates': 99.75811767578125},\n",
       " {'frame_number': 365, 'crop_coordinates': 99.66751098632812},\n",
       " {'frame_number': 366, 'crop_coordinates': 99.66009521484375},\n",
       " {'frame_number': 367, 'crop_coordinates': 100.47860717773438},\n",
       " {'frame_number': 368, 'crop_coordinates': 99.52383422851562},\n",
       " {'frame_number': 369, 'crop_coordinates': 99.49185180664062},\n",
       " {'frame_number': 370, 'crop_coordinates': 99.53372192382812},\n",
       " {'frame_number': 371, 'crop_coordinates': 102.02169799804688},\n",
       " {'frame_number': 372, 'crop_coordinates': 99.8406982421875},\n",
       " {'frame_number': 373, 'crop_coordinates': 100.67538452148438},\n",
       " {'frame_number': 374, 'crop_coordinates': 101.8046875},\n",
       " {'frame_number': 375, 'crop_coordinates': 101.735595703125},\n",
       " {'frame_number': 376, 'crop_coordinates': 102.65423583984375},\n",
       " {'frame_number': 377, 'crop_coordinates': 102.52154541015625},\n",
       " {'frame_number': 378, 'crop_coordinates': 101.47872924804688},\n",
       " {'frame_number': 379, 'crop_coordinates': 101.85406494140625},\n",
       " {'frame_number': 380, 'crop_coordinates': 101.52023315429688},\n",
       " {'frame_number': 381, 'crop_coordinates': 103.35418701171875},\n",
       " {'frame_number': 382, 'crop_coordinates': 86.48330688476562},\n",
       " {'frame_number': 383, 'crop_coordinates': 86.51934814453125},\n",
       " {'frame_number': 384, 'crop_coordinates': 86.46603393554688},\n",
       " {'frame_number': 385, 'crop_coordinates': 86.46066284179688},\n",
       " {'frame_number': 386, 'crop_coordinates': 86.52783203125},\n",
       " {'frame_number': 387, 'crop_coordinates': 119.69338989257812},\n",
       " {'frame_number': 388, 'crop_coordinates': 101.5888671875},\n",
       " {'frame_number': 389, 'crop_coordinates': 86.49453735351562},\n",
       " {'frame_number': 390, 'crop_coordinates': 86.43084716796875},\n",
       " {'frame_number': 391, 'crop_coordinates': 86.5238037109375},\n",
       " {'frame_number': 392, 'crop_coordinates': 102.4200439453125},\n",
       " {'frame_number': 393, 'crop_coordinates': 86.60537719726562},\n",
       " {'frame_number': 394, 'crop_coordinates': 86.58892822265625},\n",
       " {'frame_number': 395, 'crop_coordinates': 102.56527709960938},\n",
       " {'frame_number': 396, 'crop_coordinates': 102.63870239257812},\n",
       " {'frame_number': 397, 'crop_coordinates': 102.68341064453125},\n",
       " {'frame_number': 398, 'crop_coordinates': 103.60757446289062},\n",
       " {'frame_number': 399, 'crop_coordinates': 103.63671875},\n",
       " {'frame_number': 400, 'crop_coordinates': 102.58297729492188},\n",
       " {'frame_number': 401, 'crop_coordinates': 102.75909423828125},\n",
       " {'frame_number': 402, 'crop_coordinates': 102.759521484375},\n",
       " {'frame_number': 403, 'crop_coordinates': 102.73287963867188},\n",
       " {'frame_number': 404, 'crop_coordinates': 102.59243774414062},\n",
       " {'frame_number': 405, 'crop_coordinates': 120.84906005859375}]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(final_output, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}